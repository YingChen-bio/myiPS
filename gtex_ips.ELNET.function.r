###########################################################
## 2020.05.13 ELENET module for cell type classifcation ###
## author: Ying Chen					###
## email: yingchen.off@gmail.com			###
###########################################################



subfunc_nestedcv_scheduler <- function(K, K.start, K.stop, k.start, k.stop, n.cv.folds, n.cv.inner.folds){
  
  # Correctly stop nested CV
  if(K > K.start && K < K.stop) {
    k.start <- 0
    n.cv.inner.folds <- n.cv.folds 
    
  } else {
    if(K == K.start) {
      if(K.start != K.stop) { # && k.start != 0){
        n.cv.inner.folds <- n.cv.folds # k inner goes to .5
      } else { # K == K.start == K.stop && k.start == 0  
        n.cv.inner.folds <- k.stop
      }
    } else { # K == K.stop
      if(k.start != 0) k.start <- 0
      n.cv.inner.folds <- k.stop
    } 
  }
  res <- list(k.start = k.start, 
              n.cv.inner.folds = n.cv.inner.folds
  )
  return(res)
}

#### Alpha-grid tuning with concurrent lambda tuning --------------------------------------------------------------------------------------------------------------------------------------------------

# Please note: in the glmnet vignette: `standardize` is a logical flag for `x` variable standardization, prior to fitting the model sequence. 
# The coefficients are always returned on the original scale. Default is `standardize=TRUE`.

##### Subfunction alpha-lambda tuner version 2 (v2) using nested parallelism with mclapply * cv.glmnet() ------------------------------------------------------------------------------------------------------------------------------------

subfunc_cvglmnet_tuner_mc_v2 <- function(x, # needs to be a matrix object # df gives error!Â´
                                         y, 
                                         family = "multinomial", 
                                         type.meas = "mse", 
                                         alpha.min = 0, alpha.max = 1, by = 0.1, 
                                         n.lambda = 100, 
                                         lambda.min.ratio = 10^-6, 
                                         nfolds = 10, 
                                         balanced.foldIDs = T, # use balanced foldIDs (c060 package)
                                         seed = 1234, 
                                         parallel.comp = T, 
                                         mc.cores=2L){
  # Alpha grid
  alpha.grid <- as.list(seq(alpha.min, alpha.max, by))
  
  # Fixed foldIDs are needed so that the runs are comparable between alpha settings # see help & vignette cv.glmnet
  message("Setting balanced foldIDs with nfold = ", nfolds, " @ ", Sys.time())
  set.seed(seed)
  if(balanced.foldIDs){foldIDs <- balancedFolds(class.column.factor = y, cross.outer = nfolds)} 
  # c060::balancedFolds() => "::" sometimes caused crash with parallel
  # {foldIDs <- sample(1:nfolds, size = length(y), replace = TRUE)} 
  
  # Define new version/repurposed `cv.glmnet()` function parellized for the alpha grid 
  parallel.cvglmnet <- function(i){
    message("Tuning cv.glmnet with alpha = ", i, " @ ", Sys.time())
    set.seed(seed+1, kind ="L'Ecuyer-CMRG")
    # Inner loop of nlamba CV also using foreach %dopar% within cv.glmnet
    cv.glmnet(x = x, 
              y = y, 
              alpha = i, 
              family = family, 
              type.measure = type.meas,                 
              nlambda = n.lambda, 
              lambda.min.ratio = lambda.min.ratio,
              foldid = foldIDs,
              parallel = parallel.comp) # still allow nested parallelism (foreach) for CV of nlambda
  }
  
  # Outerloop using mclapply on the alpha[[i]] and runs parallel.cv.glmnet() function that is also parallelized with foreach %dopar% for 10fold lambda CV
  # Hence the name "crazy" nested parallelism ==> this was the fastest implementation; 
  # See also blog post by Max Kuhn: <http://appliedpredictivemodeling.com/blog/2018/1/17/parallel-processing> ; <https://topepo.github.io/caret/parallel-processing.html>
  message("Start mclapply ", " @ ", Sys.time())
  cvfit.l <- mclapply(alpha.grid, parallel.cvglmnet, 
                      mc.preschedule = T, mc.set.seed = T, mc.cores = mc.cores)  
  
  
  return(cvfit.l)                                                                
}


#### Extract alpha and lambda_1se values with lowest cv.error -----------------------------------------------------------------------------------------------------------------------------------------

subfunc_glmnet_mse_min_alpha_extractor <- function(resl, # resl is the output list of length (alpha.grid) #  generated by subfunc_cvglmnet_tuner_mc_v2()
                                                   lambda.1se = T, # if lambda.1se = F => lambda.min will be used
                                                   alpha.min = 0, 
                                                   alpha.max = 1){  
  outl <- list(length(resl))
  if(lambda.1se){
    outl <- sapply(seq_along(resl), function(i){ 
      resl[[i]]$cvm[resl[[i]]$lambda == resl[[i]]$lambda.1se] 
      # $cvm is the cross validated measure in our case MSE 
      # gets the smallest $cvm @ the lambda.1se location
    }) 
  } else {
    outl <- sapply(seq_along(resl), function(i){
      resl[[i]]$cvm[resl[[i]]$lambda == resl[[i]]$lambda.min]
    })
  }
  # ID of smallest $cvm measure ("mse") @lambda.1se accross all CV alphas-lambda model pairs
  l <- which.min(outl)          # which.min only operates on vectors => sapply() 
  # Get alpha value with above ID
  alphas <- seq(alpha.min, alpha.max, length.out = length(resl)) # assuming equal length/by division within alpha range min-max 0-1 => 0.1 => 11
  opt.alpha <- alphas[[l]]
  
  # Extracts optimal lambda either .1se (suggested by GLMNET vignette more robust estimate see. Breiman 1984) or at lambda.min
  if(lambda.1se){opt.lambda <- resl[[l]]$lambda.1se}
  else{opt.lambda <- resl[[l]]$lambda.min}
  
  # Gets directly the ID (l) with min $cvm => needed for predict()
  opt.model <- resl[[which.min(outl)]] # same as resl[[l]]
  
  # Results
  res <- list(cvm.alpha.i.list = outl, 
              opt.id = l, 
              opt.alpha = opt.alpha, 
              opt.lambda = opt.lambda, 
              opt.mod = opt.model) # model object `glmnet.fit` is => ext.cvfit.v2$opt.mod$glmnet.fit
  return(res)
}

trainGLMNET <- function(y, 
                        betas, 
                        seed = 1234, 
                        nfolds.cvglmnet = 10, 
                        mc.cores = 2L,
                        parallel.cvglmnet = T,
                        alpha.min = 0, alpha.max = 1, by = 0.1){
  
  ## 1. Train RF for variable selection
  set.seed(seed,kind = "L'Ecuyer-CMRG") 
  message("seed: ", seed)
  message("n: ", nrow(betas))  # n_patients
  message("cores: ", mc.cores)
  #getOption("mc.cores", mc.cores)#paste0(mc.cores, "L")) # getOption("mc.cores", detectCores()-2L)
  
  message("Start (concurrent) tuning of cv.glmnet hyperparameters alpha and lambda @  ", Sys.time())
  t1 <- system.time(
    cvfit.glmnet.tuning <- subfunc_cvglmnet_tuner_mc_v2(x = betas, 
                                                        y = y, 
                                                        seed = seed, 
                                                        nfolds = nfolds.cvglmnet,
                                                        family = "multinomial", 
                                                        type.meas = "mse", 
                                                        alpha.min = alpha.min, alpha.max = alpha.max, by = by, 
                                                        n.lambda = 100, 
                                                        lambda.min.ratio = 10^-6,
                                                        balanced.foldIDs = T,
                                                        mc.cores = mc.cores, # mc.cores is for the mclapply() shuffling through the alpha grid deafult 0-1 (x11)
                                                        parallel.comp = parallel.cvglmnet)   # argument for cv.glmnet it uses foreach() if parallel = T
  )
  
  # Extract permutation based importance measure # USE Version 2 of extractor!
  res.cvfit.glmnet.tuned <- subfunc_glmnet_mse_min_alpha_extractor(cvfit.glmnet.tuning, alpha.min = alpha.min, alpha.max = alpha.max)
  
  # Output hyperparameter results in the console
  message("Hyperparameter Tuning Results:", 
          "\n Optimal alpha: ", res.cvfit.glmnet.tuned$opt.alpha,
          "\n Optimal lambda: ", res.cvfit.glmnet.tuned$opt.lambda)
  
  message("Re-fitting optimal/tuned model on data @ ", Sys.time())
  t2 <-  system.time(
    probs.glmnet.tuned <- predict(object = res.cvfit.glmnet.tuned$opt.mod, 
                                  newx = betas, 
                                  s = res.cvfit.glmnet.tuned$opt.mod$lambda.1se, 
                                  type = "response")[,,1] # predict.glmnet() generates an array => [,,1] 
  ) 
  
  # Results
  res <- list(res.cvfit.glmnet.tuned$opt.mod, 
              probs.glmnet.tuned,  
              res.cvfit.glmnet.tuned, 
              t1, t2) 
  return(res)
}


run_nestedcv_GLMNET <- function(y.. = NULL,
                                betas.. = NULL,
                                path.betas.var.filtered =  "./data/gtex_ips.varfilt.5k/",  
                                fname.betas.p.varfilt = "gtex_ips.K.k",  
                                subset.CpGs.1k = F,
                                n.cv.folds = 5, 
                                nfolds.. = NULL,
                                K.start = 1, k.start = 0,
                                K.stop = NULL, k.stop = NULL,                          
                                n.cv.folds.cvglmnet = 5,
                                alpha.min. = 0, alpha.max. = 1, by. = 0.1,  
                                cores = 11, 
                                seed. = 1234, 
                                out.path = "GLMNET",
                                out.fname = "CVfold"){
  # Check:
  # Check whether y.. is provide y.. should be the label of the tissue
  gtex_anno_tissue <- readRDS("gtex_ips_anno.rds")
  colnames(gtex_anno_tissue) <- c("ID","Tissue")
  y.. <- as.factor(gtex_anno_tissue$Tissue)
  y <- y.. 
  
  # Check whether nfolds.. is provided
  nfolds.. <- readRDS("./data/nfolds.rds")
  nfolds <- nfolds..

  
  # Check whether betas.. is provided and is a matrix object <CAVE>: glmnet needs matrix not df
  if(!is.null(betas..) || class(betas..) != "matrix"){
    message("betas.. is either NULL or a 'matrix' object: TRUE")
  } else {
    stop("Please provide a matrix object for argument `betas..` ")
  } 
  
  # Feedback messages
  # Check if K.stop & k.stop was provided
  if(is.null(K.stop) && is.null(k.stop)) {
    message("\nK.stop & k.stop are at default NULL => the full `n.cv.folds` = ", n.cv.folds, 
            " nested CV will be performed.")
    K.stop <- n.cv.outer.folds <- n.cv.folds
    k.stop <- n.cv.inner.folds <- n.cv.folds
  } else { # !is.null() # NOT NULL
    message("K.stop & k.stop are provided & nested CV is limited accordingly.")
    n.cv.outer.folds <-  K.stop
    n.cv.inner.folds <- k.stop
  }
  
  # Start nested CV scheme:
  # Outer loop
  for(K in K.start:n.cv.outer.folds){
    
    # Schedule/Stop nested CV
    ncv.scheduler  <- subfunc_nestedcv_scheduler(K = K, 
                                                 K.start = K.start, K.stop = K.stop, 
                                                 k.start = k.start, k.stop = k.stop, 
                                                 n.cv.folds = n.cv.folds, 
                                                 n.cv.inner.folds = n.cv.inner.folds)
    k.start <- ncv.scheduler$k.start
    n.cv.inner.folds <- ncv.scheduler$n.cv.inner.folds
    
    # Inner/Nested loop
    for(k in k.start:n.cv.inner.folds){
      
      if(k > 0){ message("\n \nCalculating inner/nested fold ", K,".", k,"  ... ",Sys.time())  # Inner CV loops 1.1-1.5 (Fig. 1.)
        fold <- nfolds..[[K]][[2]][[k]]  
      } else{                                                                          
        message("\n \nCalculating outer fold ", K,".0  ... ",Sys.time()) # Outer CV loops 1.0-5.0 (Fig. 1.)
        fold <- nfolds..[[K]][[1]][[1]]    
      }
      
      # Default is betas.. = NULL => Loads data from path
      if(is.null(betas..)) {
        # Load pre-filtered normalized but non-batchadjusted betas for fold K.k
        message("Loading pre-filtered normalized but non-batchadjusted betas for (sub)fold ", K, ".", k)
        # Safe loading into separate env
        env2load <- environment()
        # Define path (use defaults)
        path2load <- file.path(path.betas.var.filtered) # file.path("./data/betas.var.filtered/") # default 
        fname2load <- file.path(path2load, paste(fname.betas.p.varfilt, K, k, "RData", sep = "."))  
        # Load into env
        load(file = fname2load, envir = env2load)
        # Get 
        betas.train <- get(x = "gtex.p.filtered.K.k.train", envir = env2load)
	betas.train <- as.matrix(betas.train)
        betas.test <- get(x = "gtex.p.filtered.K.k.test", envir = env2load)
	betas.test <- as.matrix(betas.test)
        # Note that betas.train and betas.test columns/CpGs are ordered in deacreasing = T => simply subset [ , 1:1000] => 1k most variable
        if(subset.CpGs.1k) {
          betas.train <- betas.train[ , 1:1000]
          betas.test <- betas.test[ , 1:1000]
        }
      } else { # User provided `betas..` (e.g. `betas2801.1.0.RData`) including (`betas2801`) both $train & $test 
        # (only for a given fold => set K.start, k.start accordingly)
        message("\n<NOTE>: This is a legacy option. The `betas.. object should contain variance filtered cases of 2801", 
                " cases according to the respective training set of (sub)fold ", K, ".", k, ".", 
                "\nThis option should be used only for a single fold corresponding to ", K.start, ".", k.start)
        betas.train <- betas..[fold$train, ] 
        betas.test <- betas..[fold$test, ]
        # If subset to 1k TRUE 
        if(subset.CpGs.1k) {
          betas.train <- betas.train[ , 1:1000]
          betas.test <- betas.test[ , 1:1000]
        }
      }
      
      # trainGLMNET
      message("Start tuning on training set using trainGLMNET function ... ", Sys.time())
      # Run train-myTunedCV.GLMNET on all $TRAIN sets both outer and inner folds => "Train the model on train set"
      glmnetcv.tuned <- trainGLMNET(y = y..[fold$train], 
                                    betas = betas.train,  
                                    seed = seed., 
                                    alpha.min = alpha.min., 
                                    alpha.max = alpha.max., 
                                    by = by.,  
                                    nfolds.cvglmnet = n.cv.folds.cvglmnet,  # 5 computationally more tractable # nfolds for cv.glmnet default = 10
                                    mc.cores = cores)
      # NOTE: glmnetcv.tuned variable contains res <-list(res.cvfit.glmnet.tuned$opt.mod, probs.glmnet.tuned, res.cvfit.glmnet.tuned, t1, t2)  
      
      # Test/Calibration set
      
      # Use tuned glmnet modell fit glmnetcv.tuned[[1]] = res.cvfit.glmnet.tuned$opt.mod => to predict the corresponding CALIBRATION or TEST SETS 
      # (if innerfold then calibration set ; if outer fold then test set)
      message("\nFit tuned glmnet on : test set ", K, ".", k, " ... ", Sys.time())
      probs <- predict(glmnetcv.tuned[[1]], 
                       newx = betas.test,
                       type="response")[,,1] # Note: the output of predict.glmnet() is an array 
      
      # Calculate Misclassification Errors (ME)
      err.probs.glmnet <- sum(colnames(probs)[apply(probs, 1, which.max)] != y..[fold$test]) / length(fold$test)
      # Print MEs
      message("\nMisclassification error on [Test Set] CVfold.", K, ".", k,
              "\n of GLMNET with alpha = ", glmnetcv.tuned[[3]]$opt.alpha, 
              " and lambda = ", glmnetcv.tuned[[3]]$opt.lambda, 
              " setting is: ", err.probs.glmnet, 
              " @ ", Sys.time())
      
      # Create output directory  
      folder.path <- file.path(getwd(), out.path)
      dir.create(folder.path, showWarnings = F, recursive = T)
      #RData.path <- file.path(folder.path, paste(out.fname, K, k, "RData", sep = "."))
      
      # Save scores, RF-Modell, fold
      save(probs, 
           glmnetcv.tuned, 
           fold,
           file = file.path(folder.path, paste(out.fname, K, k, "RData", sep = "."))
      )     
    }
  }
  message("Finished @ ",Sys.time())
}
